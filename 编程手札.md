# Lab 1

看说明文档

实现Worker()，循环要task

意识到要分类型处理task

不知道怎么按键值对读intermediate files -> 实验网站提示用json

基本完成worker逻辑，看网站的提示补漏 -> 输出文件要放在某目录

碰到go module导致的问题，学习ing...就是go.mod所在的那个文件目录在被引用的时候可以视作mod identifier



bug：

plugin.Open("wc"): plugin was built with a different version of package runtime

运行mrSequential出的，跟module有关

发现go版本落后了，更新成1.15.8，还是不行

发现自己run的时候忘记输 -race ……补上就可以了



转到coordinator

tasks用什么数据结构存储？map？slice？先用简单的map吧

需要分阶段，需要后台进程监控推进阶段

实现两个RPC handler

文档：实现worker exit，联系不上视作coordinator exit

文档：coordinator要监控，长时间未完成则re-assign

处理重复完成tasks（finishedTasks map刚好去重）

文档：[临时文件名](https://golang.org/pkg/io/ioutil/#TempFile)，os.Rename相当于重命名+迁移

修改test文件为遇到failure就退出



bug：

shell脚本执行错误 ```$'\r':command not found```

测试脚本居然是windows下写的……用了[dos2unix](https://askubuntu.com/questions/966488/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl)转成了unix版本，可以跑了



bug：

切片总是忘记/搞不清楚初始化，尤其二维切片



bug：

race bug，有些细小的地方会忘记加锁



bug：

找不到intermediate和mr-out文件

打log发现是rename迁移失败了：```%!(EXTRA []interface {}=[rename /tmp/out-516458442 mr-out-8: invalid cross-device link])```

查了发现是因为我跨磁盘移动了，系统默认放在sda，但我这个project在sdb，所以我要移动到sdb

改一下临时文件的目录即可



bug：

intermediate文件正常，mr-out文件为空

找了半天发现又是老错误，RPC参数mMap没大写开头，解析不出来，导致做reduce task的时候压根没读intermediate文件



pass all

# Lab 2

## 2A

看说明文档

实现Make()。raft peer该包含的状态依据fig 2。后台协程选举官方都设置好了，填充就行。

实现周期计时，根据文档，超时范围暂时设为200~350ms。

检查是否选举的周期暂时设为50ms。

开始选举。

call RPC需要并发，所以单独用一个函数写，每次开一个新协程call。

实现RequestVote handler。

5.4.1中解释了more up-to-date的具体定义。

candidate也要周期检查是否超时，避免刚好二分造成不动局势。

选举成功，变成leader，开始发送心跳。

心跳周期暂时设为100ms。

发给每个server的args都不同。

call仍然并发。

实现AppendEntries。

appendentries中，candidate要转换为follower，实现方法是，只要term>=currentTerm就转换（跟requestVote中的>不同）。

检查entries时要注意越界。

empEntry也要初始化，因为需要比较。

注意resetTimer的时机。

文档要求实现GetState()。



bug：

servers disagree on term

自己call自己，导致把自己转为follower了，导致不断选举新leader。

不要call自己。



bug：

expected one leader, got none

每个人都变成candidate因此无人当选，如此一直重复。

发现是因为定义electionTimeout时忘记将duration返回的值*millionsecond了……

投票后忘记更新votedFor了。

transToFollower时要将votedFor初始化，不然没法给下一轮的其他候选人投票。

发现非leader也在发送心跳，发送心跳的loop忘记判断leader身份了，并且发现不是leader了要退出这个loop，下次成为leader时再开启新的loop。



pass all 20 times



发现RPC msg的数量和bytes都比官方大不少：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1623922053911-3b200c36-3fe6-496e-bed8-3a5196942be1.png)

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1623922074301-bdd2c0e7-671f-45b2-b56c-2aa438fd159f.png)

尝试在retry前加sleep 50ms。没啥用。

尝试在retry前判断role，减少了一些！但还是两倍左右。

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1623922628659-afec3a48-cb0a-46d2-a1c2-d3c2338c1d51.png)

细细看了第一个test的log，并没有什么多余的RPC call，所以可能是我心跳比官方频繁，但心跳设置的是100ms，不算小了。试试120ms吧。并把retry周期增加为80ms。

反而变大了。。。

而且bytes是怎么回事呢……

算了 先不管了 还是100ms和50ms



## 2B

文档：实现Start()

append之后立即开始一次心跳，提前优化了。

文档：实现AppendEntries

突然理解了last new entry其实指的就是刚刚处理完的log的最后一个entry而已。

处理reply false时有一点优化是，just decrement nextIndex，然后等待下一次心跳发送，而不是在这个协程里重试，否则会有很多冗余的RPC msg。

更新commitIndex。

要注意后面引入快照之后entry.index与log的下标是有区别的，容易出错。

for循环检查matchIndex，比较笨。

reply的回复，增加了判断身份。

实现apply。

又忘了follower也需要apply，补上。



bug：

TestFailAgree2B one(102) failed to reach agreement

leader自己的matchIndex忘记更新了，导致disconnect一个server后就无法达到majority。

把判断条件改成majority-1最方便。



bug：

TestFailAgree2B index out of range [6] with length 2

AppendEntries判断prevEntry是否相同时，忘记考虑follower压根没有这个index的情况了。



bug：

TestFailAgree2B index out of range [-1]

append fail后，要减少nextIndex，不能直接rf.nextIndex[server]--，而应该将rf.nextIndex[server]置为这次append的nI-1（由于是并发的，rf.nextIndex[server]可能已经被修改过，因此这两个值可能是不同的，后者才符合逻辑）。



pass all 230 times



优化：

还是RPC msg数量太多，比如TestFailAgree2B，官方64，我跑的200左右。

分析一下：

发送RPC req的场景：

1. candidate发送投票请求 22个

根据log发现，经常出现多个candidate的情况，导致投票请求数量过多。随机范围是150ms，并不小，所以可能是checkElectionInterval过大了。调整为20ms。

某server被disconnect之后，会不断成为candidate发送投票请求，这个频率取决于electionTimeout的值，这个值目前范围是200~350ms，并不小。但是，即使开始新一轮选举，之前的投票请求仍然会不断retry，这是多余的，因此在retry前添加term判断。

reconnect之后，leader会被转为follower，但因为一直没更新timer，所以会立刻成为candidate，请求投票，然后顺利再次成为leader。

降到15了。

2. leader发送心跳 189个

正常心跳频率是100ms，计算得正常心跳应该产生50*2个masg。

每次append new entry后立即心跳一次，算下来产生14个msg。

修改连接失败后不直接retry，而是等待下次心跳。因为连接失败需要很久才回复，这时早就已经心跳了快十次了，entries也跟着更新了很多了，不如直接让心跳来替代retry。

发现reconnect->leader发送心跳，被变成follower，replicate失败->leader重新变成leader，再发送心跳，此时应该出现log inconsistency，然后leader需要更新nextIndex。这里出了问题，原本的实现是更新成args.Entries[0].Index - 1，但entries为空时就gg……修改为args.PrevLogIndex。

一下子就降到110了！

整体降到了120左右。

官方怎么可能才64呢……光心跳就有100个左右了……

不管了，目前已经没有冗余的req了，优化结束。



## 2C

用Persister模拟disk。

文档：Complete the functions persist() and readPersist()

文档：Insert calls to persist()

文档：优化nextIndex的回退，参考student guide（这里个人认为是最难理清楚的部分，然而官方直接给了答案）

1. follower不包含prevlogindex

将prevlog设为follower的最后一个log

1. follower包含prevlogindex，但term冲突，leader有follower的这个term

将prevlog设为leader的这个term的最后一个log

1. follower包含prevlogindex，但term冲突，leader没有follower的这个term

将prevlog设为follower的这个term的第一个log之前，也就是要follower完全丢弃这个term的entries（毕竟leader压根没有这个term，follower当然至少要丢弃的）



bug：

commit wrong index

发现是restart后并没有恢复之前的log，进而发现是因为Decode()的参数需要是指针…



bug：

oneHeartbeat index out of range [-1]

又把nextIndex搞成0了（entry 0都是一样的，所以nI应该始终>0）

原因如下：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624005453308-b61ecdca-3a6a-4fdb-a68e-1a5174db4f0d.png)

另外发现follower回复的success是false，然而conflict都为0，甚至follower那边都没处理这个消息。

好家伙，一个大坑，重复声明了 ```i```：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624008157653-1fa90658-216c-4060-88d7-3d0dc2f50be5.png)

处理reply前添加了以下判断就过了：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624016588130-eb3a923e-8f3e-469a-ad8c-064f74ad53e2.png)

看log看了很久终于发现了原因：

1. 已知leader为0，某follower为3
2. 0->3 AppendEntries，args.term=70
3. 由于网络延迟，3过了很久才收到这个call，此时3的term已经成为了71，因此它拒绝了这个call，reply.term=71，其他参数为默认值，因此reply.success=false，reply.conflictTerm=0，reply.conflictIndex=0

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624252288597-5f227330-7006-4393-b059-7d385762e3a0.png)

4. 0收到回复，此时0的term一定>=71，因此会处理reply，再根据上述所有默认值，会将nextIndex[3]置为0

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624252389260-df4fef2b-03ae-4aaf-a1e7-d5265eed83d7.png)

5. 因此，在处理reply之前，需要判断一下term

发现structure文档其实有提到这个坑。



pass all 900 times



## 2D

2C只是持久化log而已，2D才是要实现快照功能，即持久化app state。如此一来，只需持久化快照之后的log部分即可。restart=snapshot+log。

这一块儿的调用逻辑比较复杂。官方画的[raft_diagram](https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf)很清晰明了，对理解接口之间的调用关系有很大帮助。



首先，service会定期主动执行快照，通过Snapshot()将快照发给raft，raft修剪log并更新lastincludedxxx后，持久化raft state+snapshot。

service不自己持久化快照的原因是，反正都要告知raft去修剪log，干脆让raft一起持久化。

lastincludedxxx要持久化。

每次修剪log，将快照末entry置为log首entry，充当dummy entry。

修改所有涉及log index的地方：

- rf.log[]
- len(rf.log)：注意这里涉及如何初始化lastIncludedIndex

注意要先修剪log再更新lastincludedindex，不然log不会变化。



pass TestSnapshotBasic2D



有时某follower因为网络原因落后太多，leader发现需要发给follower的entries已经被丢掉了，会call InstallSnapshot来直接发送快照。

这里一个很重要的设计思想是，与之前的retry一样，并不立即call InstallSnapshot，而是在下次心跳时。因此，可以在心跳机制添加一个判断，若nextIndex没有被包含在当前log中，则call InstallSnapshot。

修改callAppendentries。index这块儿逻辑十分复杂……盘了好久。

修改心跳部分。

接下来的设计有些纠结。如果按照官方给的接口，在InstallSnapshot handler中并不更新raft state，只是向service发送快照，然后等待service取出快照后，再通过CondInstallSnapshot告知raft，然后更新raft state并持久化，然后更新raft的nextIndex和matchIndex。这种设计保证了service和raft能原子性地转换为同一快照。但等待Cond的时间可能比较长，这个期间会产生多个相同的心跳，虽然最后可以通过一些判断拒绝这些重复的快照，但是否有些冗余呢？

另一种设计是，放弃Cond接口，在InstallSnapshot handler中就处理好一切，service收到快照后直接apply。这样的话，service和raft就是异步转换了，但考虑了一下好像并不影响正确性。官方也给出了这种可能：

> You are free to implement Raft in a way that CondInstallSnapShot can always return true.

采用后一种设计试试。

实现InstallSnapshot handler。注意无需考虑分段快照。

处理InstallSnapshot的reply。更新nI mI。

接受快照之后，commitIndex和lastApplied也可以直接更新了，相当于之前的都commit了。（service主动快照的不用再去更新这两个变量）

初始化raft处，commitIndex和lastapplied应该赋值为lastincluedindex。



bug：

index out of range [-10]

appendentries中，没考虑prevlogindex<lastincludedindex的情况。

这时，从prevlogindex到lastincludedindex之间的log已commit，所以一定是一致的。

可以分类处理，但更简单的方式是打回，让leader再次发送新的log过来。

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624342197233-545fae11-f97b-41b4-8153-5b4fe119ce8f.png)



pass all 90 times

pass Lab2 500 times，时间差不多330s左右

pass all 2000-times



# Lab 3

## 3A

实现client。

记录leader，减少一些试错。

实现RPC handler。需要applier。

可以参考raft的test实现applier。

> An RPC handler should notice when Raft commits its Op

怎么做到？channel，用index映射。handler这边等待执行结果，阻塞，applier那边向通道发送结果。

如果不是接收这个entry的kv，只execute op，不发送执行结果。



pass TestBasic3A



duplicate detection：需要记录op id，client id。

Get无需op id，可以重复。

map记录latest seq num。

修改execute op部分，先判断是否已执行过。

> One way to do this is for the server to detect that it has lost leadership, by noticing that a different request has appeared at the index returned by Start(), or that Raft's term has changed.

handler要判断执行结果是不是属于自己这个req，如果不是，代表自己不是leader了。



bug：

Operations completed too slowly 51.30928ms/op > 33.333333ms/op

（本质是优化）

commit并apply的速度太慢了，官方只跑了15s，我这跑了50s。

apply to service部分，看log好像有点慢了，减少到40ms之后时间缩减到40s，但还是很大。又仔细看了一下student guide，意识到其实可以在更新commitindex之后直接apply，而不是loop check。于是将apply函数修改了一下，在每次commitindex更新后go apply()。

哇！！时间一下子缩短到了2s……真夸张！

重新跑lab2，好多test也变快了很多很多，不过2d第一个就死锁了：

- raft lock applyEntries，阻塞发送applych
- kv Snapshot 想要rf lock，阻塞接收applych

向applych发送时取消加锁，但这样一来，就可能出现lastapplied更新问题导致的重复apply，说白了就是apply的顺序无法保障了。

其实go apply导致多个协程可以并发更新lastapplied，已经无法保证顺序了。

还是原来的单个协程loop模式，但改成通道阻塞等待更新，每次更新commitindex后就向通道发送一个数据，触发apply，而非定期loop，这样也能起到立刻更新的效果。

pass lab2，而且比之前快了好多好多，总时间310s左右。



bug：

死锁

复盘：

- kv lock Get
- want rf lock Start
- rf lock callAppendEntries，rf.informApplyCh <- struct{}{}
- apply这边informApplyCh满了，want rf lock，阻塞

将rf.informApplyCh <- struct{}{}用单独的goroutine去执行，这样也无需缓冲区了，不过可以适当开几个缓冲位置。



bug：

TestManyPartitionsOneClient3A无法停止

这个test的测试内容：

- 开启nclient个goroutine spawn_clients_and_wait，不断发送req
- 开启一个goroutine partitioner，不断分区
- sleep 5s
- 停止上述所有goroutine
- connect all
- 等待所有req结束
- 检查结果

发现是j := <-clnts[i]这里收不到，即spawn_clients_and_wait没有向该通道发送消息。由于该协程应该在结束后自动发送消息，因此表明该协程无法停止，原因是有req没收到回复，不能返回。
看log发现，最后一个req没有被commit，因此无法apply，也就无法给client回复。

再检查发现是因为，该entry被append的时候，term是1，但经过多次分区，leader虽然因为拥有最新的log而重新被选为leader，但term变成3了，因此即使最后成功复制了该entry，也不能提交（figure 8）。

想起文档中有提醒过，需要发现leader的term改变了的情况，并让client换server重新发送req，我没有实现这个，只检查了同个index返回不同req的情况。

因此，在handler处添加超时机制，若req超过1s没回复，就当做失败，重新发送。



## 3B

修改applier，检查size太大时要快照。

初始化时要读快照。



bug：

TestSnapshotRecoverManyClients3B slice bounds out of range [-2:]

复盘：

- raft 3 -> 2 snapshot, index = 3125
- raft 2将该snapshot通过applych传给service
- service按顺序接收applych的op，处理完index=3123的op后发现raft size过大，主动执行了快照，将index=3123的快照传给了raft 2，导致raft 2出现了index=3123，lastincludedindex=3125的情况，导致切片越界
- 解决办法：raft应用快照前先判断是否stale

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624776523746-23b2b30c-6744-4bf0-a722-2144d9d6e8ee.png)



bug：

TestSnapshotRecoverManyClients3B 死锁

复盘：

- rf 1 lock InstallSnapshot
- applych <- msg
- kv 1 doSnapshot want rf lock，无法接收applych
- 构成死锁

解决办法：InstallSnapshot中发送msg之前就解锁

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624778194898-15450599-4925-4ef1-b7e7-efd4d4129926.png)



bug：

TestSnapshotRecoverManyClients3B slice bounds out of range [-1:]

复盘：

- 上一轮apply之前，lastapplied=465，apply的index范围是466~477，
- 在这个过程中，lastapplied在一次InstallSnapshot中被更新成了474
- 结束apply之后，准备更新lastapplied之前，由于这个判断，导致lastapplied更新失败，导致下一次apply时出现lastapplied+1=475，lastincludedindex=476的情况，导致getpos时越界

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624779834322-b208b323-6e07-4cca-89a7-f0a38d8d0f1c.png)

改成：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1624781573095-d3f5ef19-6512-4029-b86a-f01abf4c8a30.png)



pass all



在kv里也添加检查快照是否陈旧的代码。



bug：

TestSnapshotRecoverManyClients3B apply越界

复盘：

- 结束一次apply，此时commitinex和lastapplied都为6381
- 从leader收到一个快照。原本的lastincludedindex为6374，leader发过来的snapshot的index为6376，因此允许InstallSnapshot，并更新lastincludedindex，commitindex，lastapplied为6376
- kv执行了6381的entry后，主动执行一次快照，发给raft，于是更新lastincludedindex为6381（这里没有更新另外两个值，因为我认为既然是kv主动执行的，那么说明是已提交的，那么另外两个值一定>=这个index）
- 出现了lastincludedindex > lastapplied的情况，越界

解决办法是，要么在主动快照的地方也更新另外两个值，要么在IntsallSnapshot中更新commitindex和lastapplied时，判断是否需要更新，后者更符合逻辑。



bug：

TestSnapshotRecoverManyClients3B  get wrong value

判断是否需要发送快照给kv的机制还是有问题。

是否接受快照取决于lastincludedindex的比较，只要比自己现在的快照新，就可以接受。

但是否将接受的快照传给kv需要另外的判断：

- 若lastincludedindex <= commitIndex：那么rf迟早会将这部分快照传给kv，甚至可能已经传给kv了，若再将这个快照传给kv，便可能导致状态回退，因此不传递，并且不更新la和ci。
- 若＞，更新，传递。

继续bug，这回是越界。

忽然意识到这里有问题。如果在这期间，raft挂了，再重启时所有都被设为latsincludedindex，那么[lastapplied, lastincludedindex]这部分的log就不会被传给kv，会导致不一致。

看log发现这次的bug是这个情况：

- la 1821，ci 1823
- apply...
- ci 1836
- install 1836
- apply end，la 1823

修改：

- 在Install中，除非lastincludedindex<lastapplied不用传递以外，其他情况还是要传递。
- 在apply之前，检查若lastapplied<lastincludedindex，则更新，ci同理，这样再不会越界，也不会重复发送了。
- 在kv处也判断是否回退。

仍然bug，miss elements，情况如下：

- 一次apply结束，la=3933，这期间lastincludedindex更新为3935，发送快照给kv
- 下次apply开始，la被更新成3935，ci本身是3950，于是apply[3935,3950]这部分log
- 在接收[3935,3950]的期间，kv收到3935的快照，判断是个stale快照，丢弃。就算不丢弃，也会导致状态回退

本质是没有顺序传递吧。

啊！只有lastincludedindex>commitindex时，才接受安装快照并更新la和ci并传递给kv，似乎就可以解决了呢。其他情况都不用接受快照，这样也不会存在raft和kv数据不一致的情况了。

bug：这次报错率低了一半，但80次还是有1次wrong value

复盘：

666 append 15

688 append 16

- 4 recv 656
- 4 install 660
- 4 read ss 660
- 4 install 662
- 4 install 667，la 667，ci 667
- 4 ci to 682
- 4 read ss
- 4 read ss，这里db理应包含15
- apply 668~682
- 4 ci 691
- apply 683~691，出现db错误，没有15

重新加了信息跑，终于发现，kv读快照顺序错了，先读了667，再读662，导致状态回退！

虽说这个可以靠给kv定义变量来判断检查，但意识到还是没法保证install中向kv发送的快照与applyentries发送log之间的顺序（要求前者在后者之前，否则会丢失数据）。

从头到尾其实一直是**顺序错乱**的问题。无法保证顺序的本质原因是，向applych发送的进程有多个（applyentries负责发送普通log，n个install负责发送快照）。

回到快照机制的要求：

1. atomically switch both the service and Raft to the snapshot
2. should refuse to install a snapshot if it is an old snapshot

一个解决办法是，在install中，发送完msg之后再解锁，这样保证了第一点原子性转换，并且要求了kv必须读了这个快照之后才能接收到新快照/新log，同时解决了上述所有问题。只是担心死锁，确实是可能死锁的，因为kv做快照阻塞了通道接收，又需要rf lock。

啊！kv做快照调用rf.Snapshot这个，其实可以并发啊！反正有判断index，即使多个协程并发执行rf.Snapshot也不会有问题，反而可能减少了执行次数。

Get PutAppend也要相应的修改，调用rf.Start时不要加锁，否则可能死锁。但是，这样就有可能出现都apply了还没创建传递res的channel的情况。试试吧，大不了让client重发。



pass TestSnapshotRecoverManyClients3B  1000-times



bug：

TestPersistPartitionUnreliable3A get wrong value，history is not linearizable

看报错意思是，先append了某个值，再get没有读到这个值。

多个goroutines用同一个clerk+不同的clientid模拟多个clients……所以一个clerk发送请求是并发的。所以给clerk加锁是对的。

只是每个goroutine（模拟一个client）是串行发送请求的，也就是上一个执行完了再开始下一个。非randomkeys，因此每个goroutine（client）只发送与自己id相同的key的请求。因此，对同一个clientid（这里也就是同一个key）的请求，后面的请求的index一定>前面的请求。

看log，get的值没有问题。debug了两天啥也没看出来……

孜孜不倦地又跑了很多次，终于跑出来一个特别的情况：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1625739718311-f007a25b-2ad7-4618-9cb2-4a26f4ffba6b.png)

返回空字串，那么只有两种可能，一是no key，二是OK。

再查log发现：

- 在分区之后，两个区的leader 0和4 都接收了一个index 81的entry
- 但当然只有大多数的那个区成功提交了，也就是0接收的那个append 1 x 1 11 y
- 合并之后，4也apply了这个entry，成功执行，但将执行结果发送给了还在等待Get 1结果的RPC handler
- 而我的Get没有进行唯一标识，因此RPC handler以为成功执行了，并将OK和空字串返回了，最终导致了这个情况

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1625739192341-08682348-3563-44c6-ac6e-89e4cac52ebd.png)

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1625739131804-537f7d73-c552-40eb-b2dd-9be457e9f67e.png)

![image.png](https://cdn.nlark.com/yuque/0/2021/png/518073/1625739293307-7a3b58c4-f1cf-4f3b-abbe-85ce134033f1.png)

解决办法是，给Get增加seqnum，但只是作为身份标识，并不进行duplicate检查。

终于过了呜呜呜呜！这是整个实验最后一个bug！这个太难找了……



经提醒，RPC handler等待1s可能太长了，反正不怕重传，改成等待500ms。



pass 2000 times，平均370s左右



# Lab 4

## 4A

虽然几乎提供好了client.go，但不喜欢官方的实现方式，还是按照lab 3中来修改一下。

直接用args当成op传递，不然每次传递的无用信息太多。不过要注意注册。

common.go提供了要实现的handlers，注意依据说明文档来实现。

最困难的部分是思考reconfig的算法，要均衡，更关键的是要deterministic。文档提示map的迭代是非deterministic。

- join：每次join可能会给好几个groups，按gid从小到大一个一个来。针对每个新group，每次让当前负责shard最多的group（若有多个，取gid小的）把自己第一个shard（按序号）给新group，直到新group达到平均值。
- leave：思路基本同上。
- move：不用balance，不过还是要创建新cfg。

要处理加入第一个group的边界情况。

处理groups数量>Nshards时的情况。此时cfg.shards中不会出现没有负责任何shard的group，但这些group是在提供服务的。



pass all



## 4B

官方提供了key -> shard的映射方法。

存储系统要提供线性接口，即要求操作在所有副本执行的顺序都是一致的，因此config change也应该放入log以在相同的点执行。

修改client.go，添加duplicate detection，其他的官方写的挺完美的。

> make your servers watch for configuration changes

看第一个test来理解要做什么。

是让shardkv做loop检测cfg change，还是让controler change之后主动通知所有shardkv呢。主动通知吧，根据经验loop检测会极大拉低效率。这样要修改controler？发现shardkv是作为controler的客户端的……

文档提示说还是用loop检测，频率<100ms。

我希望能尽快把新cfg都放进log，而不是等一个新cfg应用了再把下一个放入log（这样更新太慢了）。定义一个cfgnuminlog变量，要persist吧。

在execute op时再判断wrong group。

从applych收到newcfg，切换过去。

整理出get和lose的shards。

- get：主动索要data，一个一个shard来。
- lose：立刻停止服务，等待被索要data。

要确保每次只做一个re-config，每次change cfg之前先检查。

对于get shards，得到data后开启服务，这个时间点也需要在group中统一，因此也需要通过raft log来执行，因此让leader去ask for data，得到之后放入log。对于lose shards，失去data的时间点无需统一，因为反正在接收到newcfg的时候就立刻停止服务了，所以leader直接call所有lose方的servers，不过只有leader需要返回data，其他的删除本地data即可。

不行，如果leader还没把data放入log就挂了，换leader了，怎么办……可能还是得让所有servers去做。

- all get servers send RPC to all lose servers
- all lose leader replies data（也是避免中途换了leader）
- only get leader put reply into raft log

4B难就难在没啥文档，都需要自己设计，不像lab 2都给你设计好了。

文档说lose方可以不删除本地的data，简化设计，那就不删除了。

有可能同时期收到属于多个cfg的索要数据请求，应该只能回复自己当前在处理的cfg的。



bug：

TestStaticShards 无法停止

- make_config

- - start 3 controler，cfg = 0
  - start 3*3 shardkv，query(-1) = cfg 0，故有9个query log

- join(100)，index = 10
- join(101)
- put 10个值，check是否成功

发现shardkv迟迟不能将config从0转到1，因为忘记处理lose方是0的初始情况了。

无快照的时候，重启的kv要从头apply log，故初始cfg也应该是0，而非最新cfg。

dealwithnewcfg中，等待上一个cfg转换，会阻塞上一个cfg的data conbine，从而形成死锁。把思路改变成重新put cfg into log。



bug：

TestJoinLeave Get(1): expected: DicHdlZPcH-yIb4 received: DicHdlZPcH

在100 stop service之前，101就start service了，导致同时有两个group为同一个shard提供服务。

想想后面可能出现很多来自多个cfg的数据请求，可能会混乱。应该让lose方在stop service时整理好属于该cfg的lose data，基于get方，而不是让get方直接从其database取数据。

get方combine data之后，发送RPC通知lose方可以删除了。



增加快照内容。

要保证get方把拿到的数据放入log并apply之后再清除lose方的数据，因为get方如果挂掉再重启，可能还需要再一次读这个数据。我发现我是这么做的，但重启后的get方仍然不停要数据（即使它已经成功combine了），应该是callTransData那里的逻辑差一些判断，检查一下是否已经start service。

把lose方database里的数据也删掉。



bug：

TestConcurrent3 收不到 ch 8

分析log发现，有put data的log失败了（那个index变成了其它log），但我没有写retry机制。put data into log也是需要retry机制的。

new cfg在我的设计中也需要retry机制，加上。

修改fetch new cfg的设计。思考发现之前的设计假如收到一个new cfg太早了，把它重新扔进log，但之后仍然可能被覆盖，无解。还是改成一个cfg完成之后再fetch下一个比较好，算下来也不会慢很多。

修改Transdata这块的逻辑，如果发现数据已被清理，说明group中的其它server已经拿到了，因此等待recv data即可。

如何保证拿到的data一定能放进log并成功提交？目前非leader也要一直循环start，直到发现这个数据已经被应用了。因为可能出现当前的leader把data放进log了，却被覆盖了，然后当前leader挂掉，然后这个数据就丢失了。不不，不用，数据丢失了的话，lose方自然也不会清理数据，一直没有转换到新cfg会将新cfg再次放入log，然后再次dealwithnewcfg，则get方会再次去索要数据。

修改dealwithnewcfg，重复处理。

修改putdatatolog，不retry了。



pass all



## Challenges

raftsize大概就2000+，大头还是快照。

把nextcfg换成nextcfgnum，等用到的时候再查询内容。

看test，expected = database + dup + 其他。这些不会有什么差别，所以应该是losedata的原因。

losedata在结束时本就应该都清空，但发现似乎并没有。在研究发现是，本来清除了，但是read snapshot又读回来了。把清理动作放入log。怎么保证清除了呢？用bepoked。

pass challenge 1

pass all



bug：

TestConcurrent3 又无法停止

有个if else的地方忘记解锁了。



pass all



bug：

TestConcurrent3 get wrong value

发现是kv读了旧的快照，覆盖了之前append的值。在kv里也添加检查快照是否陈旧的代码。



pass 1000 times



debug 3B修改了raft，4B这边也要相应修改一下，调用Start时不要加锁。dosnapshot时并发。



pass 2000 times，平均97s左右



经提醒，RPC handler等待1s可能太长了，反正不怕重传，改成等待500ms。

lab 2提升到300s左右（但两次测试间隔了几个实验，所以也可能是其他的修改导致的提升）。

lab 3时间没有显著提升。

lab 4b提升到91s左右。